#!/usr/bin/env python

import io
import gzip
import re
import os
import functools
import tempfile
import subprocess as sp

import click
import sqlite3
import sys
import tarfile

import pandas as pd
import psycopg2

from BioTK import LOG, CONFIG
from BioTK.io import OBO
from BioTK.db import connect
import BioTK.io

def bulk_load(path, table, *columns):
    with open(path) as h:
        LOG.info("Bulk loading %s from %s" % (table, path))
        sp.check_output(["psql", 
            "-d", CONFIG["db.name"], 
            "-h", CONFIG["db.host"],
            CONFIG["db.user"], 
            "-c", "COPY %s (%s) FROM STDIN" % (table, ", ".join(columns))],
            stdin=h)

# FIXME add FIFO and concurrently load with psql
def bulk_load_generator(generator, table, *columns):
    with tempfile.NamedTemporaryFile("wt") as h:
        for row in generator:
            row = list(map(str, row))
            h.write("\t".join(row)+"\n")
        h.flush()
        bulk_load(h.name, table, *columns)

def ensure_inserted_and_get_index(cursor, table, key, items):
    for item in items:
        if item is not None:
            cursor.execute("SELECT * FROM {table} WHERE {key}=%s;"\
                    .format(table=table,key=key), (item,))
            try:
                next(cursor)
            except StopIteration:
                cursor.execute("INSERT INTO {table} ({key}) VALUES (%s);"\
                        .format(table=table,key=key), (item,))
    connection.commit()
    cursor.execute("SELECT {key},id FROM {table};".format(**locals()))
    return dict(cursor)

def is_empty(c, table):
    c.execute("SELECT * FROM {table} LIMIT 1".format(table=table))
    try:
        next(c)
        return False
    except StopIteration:
        return True

def mapq(q):
    cursor.execute(q)
    return dict(cursor)

def populates(*tables, check_query=None):
    # check_query is a SQL query which returns >1 rows if
    # the function does not need to be run, or 0 rows if
    # the function needs to be run.
    def decorator(fn): 
        @functools.wraps(fn)
        def wrap(c, *args, **kwargs):
            if check_query is not None:
                cursor.execute(check_query)
                try:
                    next(cursor)
                    needed = False
                except StopIteration:
                    needed = True
            elif tables:
                needed = all(is_empty(c, tbl) for tbl in tables)
            else:
                assert False

            if needed:
                LOG.info("Executing %s" % fn.__name__)
                return fn(c, *args, **kwargs)
            else:
                LOG.info("Skipping %s" % fn.__name__)
        return wrap
    return decorator

def read_dmp(handle, columns):
    """
    Read a NCBI .dmp file into a DataFrame.
    """
    buffer = io.StringIO()
    for i,line in enumerate(handle):
        buffer.write(line.rstrip("\t|\n") + "\n")
    buffer.seek(0)
    return pd.read_table(buffer, delimiter="\t\|\t", 
            names=columns,
            header=None)

def geo_connect():
    return sqlite3.connect("/data/public/geo/GEOmetadb.sqlite")

def common_taxa():
    taxa = set()
    geo_db = geo_connect()
    c = geo_db.cursor()
    q = """
        select * from (
            select organism, count(organism) as c 
            from gpl 
            group by organism
        ) where c > 10;
        """
    for row in c.execute(q):
        if ";" not in row:
            taxa.add(row[0])
    geo_db.close()
    return taxa

#########
# Loaders
#########

@populates("taxon")
def load_taxon(c):
    taxa = common_taxa()
    url = "ftp://ftp.ncbi.nih.gov/pub/taxonomy/taxdump.tar.gz"
    cached_path = BioTK.io.download(url)
    with tarfile.open(cached_path, mode="r:gz") as archive:
        h = io.TextIOWrapper(archive.extractfile("names.dmp"),
                encoding="utf-8")
        columns = ["id", "name", "_", "type"]
        data = read_dmp(h, columns)
        data = data.ix[data["type"] == \
                "scientific name",["id","name"]]
        data = data.drop_duplicates("id").dropna()
        data = data[data["name"].isin(taxa)]
        c.executemany("""
            INSERT INTO taxon (id, name) VALUES (%s,%s);
            """, ((int(id), str(name))
                for id, name in
                data.to_records(index=False)))

@populates("gene")
def load_gene(c):
    c.execute("SELECT id FROM taxon;")
    taxa = set([r[0] for r in c])

    url = "ftp://ftp.ncbi.nih.gov/gene/DATA/gene_info.gz"
    path = BioTK.io.download(url)
    with tempfile.NamedTemporaryFile("wt") as o:
        nullable = lambda x: None if x == "-" else x
        with io.TextIOWrapper(gzip.open(path, "r"), encoding="utf-8") as h:
            next(h)
            for line in h:
                fields = line.split("\t")
                taxon_id = int(fields[0])
                if taxon_id not in taxa:
                    continue
                gene_id = int(fields[1])
                symbol = nullable(fields[2])
                name = nullable(fields[11])
                row = list(map(str, (taxon_id, gene_id, symbol, name)))
                o.write("\t".join(row) + "\n")
        o.flush()
        bulk_load(o.name, "gene", "taxon_id", "id", "symbol", "name")

def load_ontology(c, prefix, name, path):
    c.execute("INSERT INTO ontology (prefix,name) VALUES (%s,%s) RETURNING id",
            (prefix, name))
    ontology_id = next(c)[0]

    with open(path) as h:
        o = OBO.parse(h)

    # Insert (if necessary) and cache namespaces
    namespace_to_id = ensure_inserted_and_get_index(c, 
            "namespace", "text", o.terms["Namespace"])
    accession_to_id = {}

    # Insert terms
    for accession, name, namespace in o.terms.to_records(index=True):
        if isinstance(namespace, list) or namespace is None:
            # FIXME
            namespace_id = None
        else:
            namespace_id = namespace_to_id[namespace]
        c.execute("""
        INSERT INTO term (ontology_id,namespace_id,accession,name)
        VALUES (%s,%s,%s,%s)
        RETURNING id;
        """, (ontology_id,namespace_id,accession,name))
        accession_to_id[accession] = next(c)[0]
                    
    # Insert synonyms
    synonyms = o.synonyms
    synonym_to_id = ensure_inserted_and_get_index(c,
            "synonym", "text", set(synonyms["Synonym"]))

    # Insert term-synonym links
    for accession, synonym in synonyms.to_records():
        term_id = accession_to_id[accession]
        synonym_id = synonym_to_id[synonym]
        c.execute("INSERT INTO term_synonym VALUES (%s,%s)",
                (term_id,synonym_id))
       
    # Insert relationships    
    relationship_to_id = ensure_inserted_and_get_index(c,
            "relationship", "name", set(o.relations["Relation"]))

    # Insert term-term links
    inserted_terms = set(o.terms.index)
    for agent, target, r in o.relations.to_records(index=False):
        agent_id = accession_to_id.get(agent)
        target_id = accession_to_id.get(target)
        if (agent_id is None) or (target_id is None):
            continue
        relationship_id = relationship_to_id[r]
        c.execute("""
        INSERT INTO term_term (agent_id,target_id,relationship_id)
        VALUES (%s,%s,%s);
        """, (agent_id, target_id, relationship_id))

OBO_PATH = "/data/public/ontology/obo/"
ONTOLOGIES = [
        ("GO", "Gene Ontology"),
        ("BTO", "Brenda Tissue Ontology"),
        ("PATO", "Phenotypic Quality Ontology")
]

@populates("ontology", "term")
def load_ontologies(c):
    for prefix, name in ONTOLOGIES:
        LOG.debug("Loading data for %s (%s)" % (prefix, name))
        load_ontology(c, prefix, name, OBO_PATH+prefix+".obo")

@populates("platform", "series", "sample")
def load_geo(c):
    geo_db = geo_connect()
    gc = geo_db.cursor()
    c.execute("SELECT name,id FROM taxon;")
    taxon_to_id = dict(c)

    source = "Gene Expression Omnibus"
    source_id = ensure_inserted_and_get_index(c,
            "source", "name", [source])[source]

    gpl_to_id = {}
    gse_to_id = {}
    gsm_to_id = {}

    # Insert GPL
    print("Insert GPL")
    gc.execute("SELECT %s,gpl,title,manufacturer FROM gpl;" % source_id)
    for r in gc:
        c.execute("""
            INSERT INTO platform (source_id,accession,title,manufacturer) 
            VALUES (%s,%s,%s,%s)
            RETURNING id;""", r)
        gpl_to_id[r[1]] = next(c)[0]

    # Insert GSE
    print("Insert GSE")
    gc.execute("""
        SELECT %s,gse,type,summary,overall_design,submission_date,title 
        FROM gse;""" % source_id)
    for row in gc:
        c.execute("""
            INSERT INTO series (source_id,accession,type,summary,
                design,submission_date,title)
            VALUES (%s,%s,%s,%s,%s,%s,%s)
            RETURNING id;""", row)
        gse_to_id[row[1]] = next(c)[0]

    # Insert GSM
    print("Insert GSM")
    gc.execute("""
        SELECT gsm,organism_ch1,gpl,%s,title,type,source_name_ch1,molecule_ch1,
            channel_count,description,characteristics_ch1
        FROM gsm;""" % source_id)
    for row in gc:
        row = list(row)
        row[1] = taxon_to_id.get(row[1])
        if not row[1]:
            continue
        row[2] = gpl_to_id[row[2]]
        row[8] = int(row[8])
        c.execute("""
            INSERT INTO sample (accession, taxon_id, platform_id, source_id,
                title, type, source_name, molecule, channel_count, 
                description, characteristics)
            VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)
            RETURNING id;""", row)
        gsm_to_id[row[0]] = next(c)[0]    
        
    # Insert GSM-GSE links
    print("Insert GSM-GSE")
    gc.execute("""
        SELECT gse_gsm.gsm,gse
        FROM gse_gsm
        INNER JOIN gsm
        ON gsm.gsm=gse_gsm.gsm;""")
    for gsm,gse in gc:
        sample_id = gsm_to_id.get(gsm)
        series_id = gse_to_id.get(gse)
        if sample_id and series_id:
            c.execute("""
                INSERT INTO sample_series VALUES (%s,%s)
                """, (gsm_to_id[gsm],gse_to_id[gse]))

############################################
# Manual or official gene/sample annotations
############################################

check_gene_go = """
    SELECT * FROM term_gene
    INNER JOIN source
    ON source.id=term_gene.source_id
    WHERE source.name='Gene Ontology Consortium'
    LIMIT 1;"""

@populates(check_query=check_gene_go)
def load_gene_go(c):
    cursor.execute("select accession,id FROM term")
    accession_to_term_id = dict(c)
    source = "Gene Ontology Consortium"
    source_id = ensure_inserted_and_get_index(c,"source","name",
            [source])[source]
    c.execute("SELECT id FROM gene;")
    genes = set(row[0] for row in c)

    url = "ftp://ftp.ncbi.nlm.nih.gov/gene/DATA/gene2go.gz"
    path = BioTK.io.download(url)
    with gzip.open(path, "r") as h:
        df = pd.read_table(h, skiprows=1, 
                header=None, names=("Taxon ID", "Gene ID", "Term ID", 
                    "Evidence", "Qualifier", "TermName", "PubMed", "Category"))
        df = df[df["Gene ID"].isin(genes)]
        evidence_to_id = ensure_inserted_and_get_index(c,"evidence","name",
                set(df["Evidence"]))
        records = ((accession_to_term_id[term_accession], int(gene_id),
                evidence_to_id[evidence], source_id)
                for gene_id,term_accession,evidence 
                in df.iloc[:,1:4].dropna().itertuples(index=False))
        bulk_load_generator(records, "term_gene",
                "term_id","gene_id","evidence_id","source_id")

check_ursa = """
    SELECT * FROM term_sample
    INNER JOIN source
    ON term_sample.source_id=source.id
    WHERE source.name='URSA'
    LIMIT 1;
    """

@populates(check_query=check_ursa)
def load_ursa(c):
    url = "http://ursa.princeton.edu/supp/manual_annotations_ursa.csv"
    path = BioTK.io.download(url)
    evidence = "manual"
    evidence_id = ensure_inserted_and_get_index(c,
            "evidence","name",[evidence])[evidence]
    source = "URSA"
    source_id = ensure_inserted_and_get_index(c,
            "source", "name", [source])[source]

    sample_accession_to_id = mapq("SELECT accession,id FROM sample;")
    c.execute("""
            SELECT term.name,term.id FROM term
            INNER JOIN ontology
            ON term.ontology_id=ontology.id
            WHERE ontology.prefix='BTO';""")
    term_name_to_id = dict(c)

    def generate():
        with open(path) as h:
            next(h)
            for line in h:
                gsm, _, bto_name = line.strip().split("\t")
                term_id = term_name_to_id.get(bto_name)
                sample_id = sample_accession_to_id.get(gsm)
                if (term_id is not None) and (sample_id is not None):
                    yield term_id,sample_id,evidence_id,source_id

    c.executemany("""
        INSERT INTO term_sample (
            term_id,sample_id,evidence_id,source_id)
        VALUES (%s,%s,%s,%s);""", generate())

######################################
# Text-mining based sample annotations
######################################

def group_if_match(pattern, group, text):
    m = re.search(pattern, text)
    if m:
        return m.group(group)

PATTERNS = {
    "age": "[^\w]age( *\((?P<age_unit1>[a-z]*)\))?:\
[ \t]*(?P<age>\d+[\.0-9]*)(( *\- *| (to|or) )\
(?P<age_end>\d+[\.\d]*))?([ \t]*(?P<age_unit2>[a-z]+))?",
    "age_unit": "(age\s*unit[s]*|unit[s]* of age): (?P<age_unit>[a-z])",
    # Tissue (TODO: map to BTO)
    "tissue": "(cell type|tissue|organ) *: *(?P<tissue>[A-Za-z0-9\+\- ]+)",
    # Disease states (TODO: map to DO)
    "cancer": "(tumor|tumour|cancer|sarcoma|glioma|leukem|mesothelioma|metastasis|carcinoma|lymphoma|blastoma|nsclc|cll|ptcl)",
    "infection": "infec"
}
PATTERNS = dict((k, re.compile(v)) for k,v in PATTERNS.items())

# A common additional unit is "dpc", which refers to embryos. 
# Currently ignored.
# Some samples are labeled with something like "11 and 14 weeks". 
# I have no idea what this means, so it's ignored. 
TIME_CONVERSION = {
        "year": 12,
        "y": 12,
        "yr": 12,
        "month": 1,
        "moth": 1, # yes...
        "mo": 1,
        "m": 1,
        "week": 1 / 4.5,
        "wek": 1 / 4.5, # ...
        "wk": 1 / 4.5,
        "w": 1 / 4.5,
        "day": 1 / 30,
        "d": 1 / 30,
        "hour": 1 / (24 * 30),
        "hr": 1 / (24 * 30),
        "h": 1 / (24 * 30)
}

def load_sample_age(c):
    c.execute("SELECT id FROM term WHERE name='age';")
    age_term_id = next(c)[0]
    rows = []
    evidence = "text mining"
    evidence_id = ensure_inserted_and_get_index(c,
            "evidence","name",[evidence])[evidence]
    source = "Wren Lab"
    source_id = ensure_inserted_and_get_index(c,
            "source", "name", [source])[source]

    c.execute("""
        SELECT st.id,taxon.id,st.text FROM sample_text st
        INNER JOIN sample
        ON sample.id=st.id
        INNER JOIN taxon
        ON sample.taxon_id=taxon.id;
        """)
    for sample_id,taxon_id,text in c:
        default_unit = "year" if taxon_id == 9606 else None
        m = re.search(PATTERNS["age"], text)
        if m is None:
            continue
        age = float(m.group("age"))
        age_end = m.group("age_end")
        if age_end:
            #if not use_age_range:
            #    continue
            age = (age + float(age_end)) / 2
        
        unit = group_if_match(PATTERNS["age_unit"], 
                "age_unit", text) \
                or m.group("age_unit2") \
                or m.group("age_unit1")
                #or default_unit
        if not unit:
            continue
        unit = unit.rstrip("s")
        if not unit in TIME_CONVERSION:
            continue
        conversion_factor = TIME_CONVERSION[unit]
        rows.append((age_term_id, sample_id, evidence_id, source_id,
                age * conversion_factor))
    c.executemany("""
        INSERT INTO term_sample 
            (term_id,sample_id,evidence_id,source_id,value) 
        VALUES (%s,%s,%s,%s,%s)""", rows)


# AILUN probe mappings

def read_ailun():
    c.execute("SELECT accession,id FROM platform;")
    platform_accession_to_id = dict(c)

    AILUN_DIR = "/data/public/geo/annotation/AILUN"
    for i,path in enumerate(os.listdir(AILUN_DIR)):
        if not path.endswith(".annot.gz"):
            continue
        platform_accession = path.split(".")[0]
        platform_id = platform_accession_to_id.get(platform_accession)
        if not platform_id:
            continue
        path = os.path.join(AILUN_DIR, path)
        mappings = []
        with gzip.open(path, "rt") as h:
            for line in h:
                try:
                    probe_name, gene_id, *_ = line.split("\t")
                    gene_id = int(gene_id)
                    mappings.append((probe_name, gene_id))
                except:
                    pass
        LOG.debug("Processing platform %s" % platform_accession)
        yield platform_id, mappings

@populates("probe")
def load_probe(c):
    with tempfile.NamedTemporaryFile("wt") as h:
        for platform_id, mappings in read_ailun():
            for probe_name, _ in mappings:
                h.write("%s\t%s\n" % (platform_id, probe_name))
        h.flush()
        bulk_load(h.name, "probe", "platform_id", "accession")

@populates("probe_gene")
def load_probe_gene(c):
    c.execute("SELECT id FROM gene;")
    genes = set(r[0] for r in c)

    with tempfile.NamedTemporaryFile("wt", delete=False) as h:
        for platform_id, mappings in read_ailun():
            c.execute("SELECT accession,id FROM probe WHERE platform_id=%s;", 
                    (platform_id,))
            probe_name_to_id = dict(c)
            pairs = set()
            for probe_name, gene_id in mappings:
                probe_id = probe_name_to_id[probe_name]
                if not gene_id in genes:
                    continue
                pair = (probe_id,gene_id)
                if not pair in pairs:
                    h.write("%s\t%s\n" % pair)
                    pairs.add(pair)
    try:
        sp.call(["sort", "-S", "4G", "-u", "-o", h.name, h.name])
        bulk_load(h.name, "probe_gene", "probe_id", "gene_id")
    finally:
        os.unlink(h.name)

if __name__ == "__main__":
    connection = connect()
    c = cursor = connection.cursor()

    load_taxon(c)
    connection.commit()
    load_gene(c)
    load_geo(c)
    connection.commit()

    #load_probe(c)
    #load_probe_gene(c)

    load_ontologies(c)
    connection.commit()
    load_gene_go(c)
    load_ursa(c)
    load_sample_age(c)
    connection.commit()
